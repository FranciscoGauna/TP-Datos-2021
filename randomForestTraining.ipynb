{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "88ce181b8cd0eb812200b9b411a136cd9c7c972aa91497a27a228178326dd49d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "import pandas as pd\r\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# for preprocessing the data\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "# the model\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "# for optimizing the hyperparameters of the pipeline\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "df_train_labels_original = pd.read_csv('train_labels.csv',low_memory=False,index_col='building_id')\r\n",
    "df_train_values_original = pd.read_csv('train_values.csv',low_memory=False, index_col='building_id', dtype= {\r\n",
    "'geo_level_1_id':'uint8', 'geo_level_2_id':'uint16', 'geo_level_3_id':'uint16', 'count_floors_pre_eq':'uint8','age':'uint16', 'area_percentage':'uint16', 'height_percentage':'uint16', \r\n",
    "'land_surface_condition':'category', 'foundation_type':'category', 'roof_type':'category', 'ground_floor_type':'category', 'other_floor_type':'category', 'position':'category','plan_configuration':'category', \r\n",
    "'has_superstructure_adobe_mud':'bool', 'has_superstructure_mud_mortar_stone':'bool','has_superstructure_stone_flag':'bool', 'has_superstructure_cement_mortar_stone':'bool', 'has_superstructure_mud_mortar_brick':'bool', 'has_superstructure_cement_mortar_brick':'bool', 'has_superstructure_timber':'bool', 'has_superstructure_bamboo':'bool', 'has_superstructure_rc_non_engineered':'bool', 'has_superstructure_rc_engineered':'bool', 'has_superstructure_other':'bool', \r\n",
    "'legal_ownership_status':'category', 'count_families':'uint16', \r\n",
    "'has_secondary_use':'bool', 'has_secondary_use_agriculture':'bool', 'has_secondary_use_hotel':'bool', 'has_secondary_use_rental':'bool', 'has_secondary_use_institution':'bool', 'has_secondary_use_school':'bool', 'has_secondary_use_industry':'bool', 'has_secondary_use_health_post':'bool', 'has_secondary_use_gov_office':'bool', 'has_secondary_use_use_police':'bool', 'has_secondary_use_other':'bool',})\r\n",
    "df_test_values_original = pd.read_csv('test_values.csv',low_memory=False, index_col='building_id', dtype= {\r\n",
    "'geo_level_1_id':'uint8', 'geo_level_2_id':'uint16', 'geo_level_3_id':'uint16', 'count_floors_pre_eq':'uint8','age':'uint16', 'area_percentage':'uint16', 'height_percentage':'uint16', \r\n",
    "'land_surface_condition':'category', 'foundation_type':'category', 'roof_type':'category', 'ground_floor_type':'category', 'other_floor_type':'category', 'position':'category','plan_configuration':'category', \r\n",
    "'has_superstructure_adobe_mud':'bool', 'has_superstructure_mud_mortar_stone':'bool','has_superstructure_stone_flag':'bool', 'has_superstructure_cement_mortar_stone':'bool', 'has_superstructure_mud_mortar_brick':'bool', 'has_superstructure_cement_mortar_brick':'bool', 'has_superstructure_timber':'bool', 'has_superstructure_bamboo':'bool', 'has_superstructure_rc_non_engineered':'bool', 'has_superstructure_rc_engineered':'bool', 'has_superstructure_other':'bool', \r\n",
    "'legal_ownership_status':'category', 'count_families':'uint16', \r\n",
    "'has_secondary_use':'bool', 'has_secondary_use_agriculture':'bool', 'has_secondary_use_hotel':'bool', 'has_secondary_use_rental':'bool', 'has_secondary_use_institution':'bool', 'has_secondary_use_school':'bool', 'has_secondary_use_industry':'bool', 'has_secondary_use_health_post':'bool', 'has_secondary_use_gov_office':'bool', 'has_secondary_use_use_police':'bool', 'has_secondary_use_other':'bool',})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "train_values_subset = pd.get_dummies(df_train_values_original)\r\n",
    "train_labels_subset = df_train_labels_original['damage_grade']\r\n",
    "\r\n",
    "validation_size = df_train_values_original.index.size - df_test_values_original.index.size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "test_values_subset = pd.get_dummies(df_test_values_original)\r\n",
    "geo1Test = pd.get_dummies(test_values_subset[\"geo_level_1_id\"])\r\n",
    "geo1TestNames = {}\r\n",
    "for x in geo1Test.columns: geo1TestNames[x] = ('geo1Test_'+ str(x))\r\n",
    "geo1Test = geo1Test.rename(geo1TestNames, axis=1)\r\n",
    "# test_values_subset = pd.concat([test_values_subset, geo1Test], axis=1)\r\n",
    "\r\n",
    "geo2Test = pd.get_dummies(test_values_subset[\"geo_level_2_id\"])\r\n",
    "geo2TestNames = {}\r\n",
    "for x in geo2Test.columns: geo2TestNames[x] = ('geo2_'+ str(x))\r\n",
    "geo2Test = geo2Test.rename(geo2TestNames, axis=1)\r\n",
    "\r\n",
    "geo2Test = geo2Test[['geo2_36', 'geo2_39', 'geo2_51', 'geo2_88', 'geo2_105', 'geo2_142',\r\n",
    "       'geo2_158', 'geo2_173', 'geo2_229', 'geo2_233', 'geo2_258', 'geo2_303',\r\n",
    "       'geo2_323', 'geo2_363', 'geo2_399', 'geo2_421', 'geo2_477', 'geo2_508',\r\n",
    "       'geo2_533', 'geo2_566', 'geo2_582', 'geo2_617', 'geo2_641', 'geo2_673',\r\n",
    "       'geo2_682', 'geo2_715', 'geo2_797', 'geo2_811', 'geo2_817', 'geo2_819',\r\n",
    "       'geo2_839', 'geo2_856', 'geo2_864', 'geo2_886', 'geo2_896', 'geo2_937',\r\n",
    "       'geo2_977', 'geo2_1001', 'geo2_1009', 'geo2_1023', 'geo2_1050',\r\n",
    "       'geo2_1074', 'geo2_1080', 'geo2_1132', 'geo2_1149', 'geo2_1155',\r\n",
    "       'geo2_1183', 'geo2_1253', 'geo2_1313', 'geo2_1401']]\r\n",
    "\r\n",
    "test_values_subset = pd.concat([test_values_subset, geo1Test, geo2Test], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "geo1 = pd.get_dummies(train_values_subset[\"geo_level_1_id\"])\r\n",
    "geo1Names = {}\r\n",
    "for x in geo1.columns: geo1Names[x] = ('geo1_'+ str(x))\r\n",
    "geo1 = geo1.rename(geo1Names, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "geo2 = pd.get_dummies(train_values_subset[\"geo_level_2_id\"])\r\n",
    "geo2Names = {}\r\n",
    "for x in geo2.columns: geo2Names[x] = ('geo2_'+ str(x))\r\n",
    "geo2 = geo2.rename(geo2Names, axis=1)\r\n",
    "geo2 = geo2[['geo2_36', 'geo2_39', 'geo2_51', 'geo2_88', 'geo2_105', 'geo2_142',\r\n",
    "       'geo2_158', 'geo2_173', 'geo2_229', 'geo2_233', 'geo2_258', 'geo2_303',\r\n",
    "       'geo2_323', 'geo2_363', 'geo2_399', 'geo2_421', 'geo2_477', 'geo2_508',\r\n",
    "       'geo2_533', 'geo2_566', 'geo2_582', 'geo2_617', 'geo2_641', 'geo2_673',\r\n",
    "       'geo2_682', 'geo2_715', 'geo2_797', 'geo2_811', 'geo2_817', 'geo2_819',\r\n",
    "       'geo2_839', 'geo2_856', 'geo2_864', 'geo2_886', 'geo2_896', 'geo2_937',\r\n",
    "       'geo2_977', 'geo2_1001', 'geo2_1009', 'geo2_1023', 'geo2_1050',\r\n",
    "       'geo2_1074', 'geo2_1080', 'geo2_1132', 'geo2_1149', 'geo2_1155',\r\n",
    "       'geo2_1183', 'geo2_1253', 'geo2_1313', 'geo2_1401']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "train_values_subset = pd.concat([train_values_subset, geo1, geo2], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "train_values, validation_values = (train_values_subset.iloc[0:173733], train_values_subset.iloc[173733:-1])\r\n",
    "train_labels, validation_labels = (train_labels_subset.iloc[0:173733], train_labels_subset.iloc[173733:-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "param_grid = {\r\n",
    "        'n_estimators': [100, 200, 300, 400],\r\n",
    "        'min_samples_split': [0.1, 0.5, 0.9],\r\n",
    "        'min_samples_leaf': [0.1, 0.3, 0.5],\r\n",
    "        'max_features': ['auto', 'log2', None],\r\n",
    "        'n_jobs': [-1],\r\n",
    "        'bootstrap':[True]}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "rs_clf = GridSearchCV(RandomForestClassifier(), param_grid, verbose=2, scoring='f1_micro', cv=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rs_clf.fit(train_values, train_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rs_clf.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "rndfor = RandomForestClassifier(n_estimators=300, bootstrap=True, n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "rndfor.fit(train_values, train_labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# predictions = rndfor.predict(test_values_subset) \r\n",
    "predictionsProba = rndfor.predict_proba(validation_values) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "predictions = rndfor.predict(validation_values) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "df_prediction_rndForest = pd.DataFrame(predictionsProba, columns = ['rndf1', 'rndf2', 'rndf3'])\r\n",
    "df_prediction_rndForest.to_csv(\"randomForestPredictionProbaTrain.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "f1_score(validation_labels, predictions, average='micro')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7203195689962817"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission_format = pd.read_csv('submission_format.csv', index_col='building_id')\r\n",
    "my_submission = pd.DataFrame(data=predictions,\r\n",
    "                             columns=submission_format.columns,\r\n",
    "                             index=submission_format.index)\r\n",
    "my_submission.to_csv('submissionRandomForest.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}