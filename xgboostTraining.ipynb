{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "import pandas as pd\r\n",
    "pd.options.display.float_format = '{:20,.2f}'.format"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# for optimizing the hyperparameters of the pipeline\r\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import xgboost as xgb\r\n",
    "from xgboost.sklearn import XGBClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "df_train_labels_original = pd.read_csv('train_labels.csv',low_memory=False,index_col='building_id')\r\n",
    "df_train_values_original = pd.read_csv('train_values.csv',low_memory=False, index_col='building_id', dtype= {\r\n",
    "'geo_level_1_id':'uint8', 'geo_level_2_id':'uint16', 'geo_level_3_id':'uint16', 'count_floors_pre_eq':'uint8','age':'uint16', 'area_percentage':'uint16', 'height_percentage':'uint16', \r\n",
    "'land_surface_condition':'category', 'foundation_type':'category', 'roof_type':'category', 'ground_floor_type':'category', 'other_floor_type':'category', 'position':'category','plan_configuration':'category', \r\n",
    "'has_superstructure_adobe_mud':'bool', 'has_superstructure_mud_mortar_stone':'bool','has_superstructure_stone_flag':'bool', 'has_superstructure_cement_mortar_stone':'bool', 'has_superstructure_mud_mortar_brick':'bool', 'has_superstructure_cement_mortar_brick':'bool', 'has_superstructure_timber':'bool', 'has_superstructure_bamboo':'bool', 'has_superstructure_rc_non_engineered':'bool', 'has_superstructure_rc_engineered':'bool', 'has_superstructure_other':'bool', \r\n",
    "'legal_ownership_status':'category', 'count_families':'uint16', \r\n",
    "'has_secondary_use':'bool', 'has_secondary_use_agriculture':'bool', 'has_secondary_use_hotel':'bool', 'has_secondary_use_rental':'bool', 'has_secondary_use_institution':'bool', 'has_secondary_use_school':'bool', 'has_secondary_use_industry':'bool', 'has_secondary_use_health_post':'bool', 'has_secondary_use_gov_office':'bool', 'has_secondary_use_use_police':'bool', 'has_secondary_use_other':'bool',})\r\n",
    "df_test_values_original = pd.read_csv('test_values.csv',low_memory=False, index_col='building_id', dtype= {\r\n",
    "'geo_level_1_id':'uint8', 'geo_level_2_id':'uint16', 'geo_level_3_id':'uint16', 'count_floors_pre_eq':'uint8','age':'uint16', 'area_percentage':'uint16', 'height_percentage':'uint16', \r\n",
    "'land_surface_condition':'category', 'foundation_type':'category', 'roof_type':'category', 'ground_floor_type':'category', 'other_floor_type':'category', 'position':'category','plan_configuration':'category', \r\n",
    "'has_superstructure_adobe_mud':'bool', 'has_superstructure_mud_mortar_stone':'bool','has_superstructure_stone_flag':'bool', 'has_superstructure_cement_mortar_stone':'bool', 'has_superstructure_mud_mortar_brick':'bool', 'has_superstructure_cement_mortar_brick':'bool', 'has_superstructure_timber':'bool', 'has_superstructure_bamboo':'bool', 'has_superstructure_rc_non_engineered':'bool', 'has_superstructure_rc_engineered':'bool', 'has_superstructure_other':'bool', \r\n",
    "'legal_ownership_status':'category', 'count_families':'uint16', \r\n",
    "'has_secondary_use':'bool', 'has_secondary_use_agriculture':'bool', 'has_secondary_use_hotel':'bool', 'has_secondary_use_rental':'bool', 'has_secondary_use_institution':'bool', 'has_secondary_use_school':'bool', 'has_secondary_use_industry':'bool', 'has_secondary_use_health_post':'bool', 'has_secondary_use_gov_office':'bool', 'has_secondary_use_use_police':'bool', 'has_secondary_use_other':'bool',})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "train_values_subset = pd.get_dummies(df_train_values_original)\r\n",
    "train_labels_subset = df_train_labels_original['damage_grade']\r\n",
    "\r\n",
    "validation_size = df_train_values_original.index.size - df_test_values_original.index.size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "test_values_subset = pd.get_dummies(df_test_values_original)\r\n",
    "geo1Test = pd.get_dummies(test_values_subset[\"geo_level_1_id\"])\r\n",
    "geo1TestNames = {}\r\n",
    "for x in geo1Test.columns: geo1TestNames[x] = ('geo1Test_'+ str(x))\r\n",
    "geo1Test = geo1Test.rename(geo1TestNames, axis=1)\r\n",
    "\r\n",
    "test_values_subset = pd.concat([test_values_subset, geo1Test], axis=1)\r\n",
    "\r\n",
    "mean_df = pd.read_csv('geolevel_2_id_mean.csv',low_memory=False)\r\n",
    "test_values_subset = test_values_subset.reset_index().merge(mean_df,on='geo_level_2_id',how='left').set_index('building_id')\r\n",
    "average_damage = pd.get_dummies(df_train_labels_original['damage_grade'][:int(len(df_train_labels_original)/2)]).mean().values\r\n",
    "test_values_subset['0'] = test_values_subset['0'].fillna(average_damage[0])\r\n",
    "test_values_subset['1'] = test_values_subset['1'].fillna(average_damage[1])\r\n",
    "test_values_subset['2'] = test_values_subset['2'].fillna(average_damage[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "geo1 = pd.get_dummies(train_values_subset[\"geo_level_1_id\"])\r\n",
    "geo1Names = {}\r\n",
    "for x in geo1.columns: geo1Names[x] = ('geo1_'+ str(x))\r\n",
    "geo1 = geo1.rename(geo1Names, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "train_values_subset = pd.concat([train_values_subset, geo1], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "mean_df = pd.read_csv('geolevel_2_id_mean.csv',low_memory=False)\r\n",
    "train_values_subset = train_values_subset.reset_index().merge(mean_df,on='geo_level_2_id',how='left').set_index('building_id')\r\n",
    "average_damage = pd.get_dummies(df_train_labels_original['damage_grade'][:int(len(df_train_labels_original)/2)]).mean().values\r\n",
    "train_values_subset['0'] = train_values_subset['0'].fillna(average_damage[0])\r\n",
    "train_values_subset['1'] = train_values_subset['1'].fillna(average_damage[1])\r\n",
    "train_values_subset['2'] = train_values_subset['2'].fillna(average_damage[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "train_values, validation_values = (train_values_subset.iloc[0:173733], train_values_subset.iloc[173733:-1])\r\n",
    "train_labels, validation_labels = (train_labels_subset.iloc[0:173733], train_labels_subset.iloc[173733:-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "param_grid = {\r\n",
    "        'max_depth': [3, 5, 6, 7, 8, 10],\r\n",
    "        'learning_rate': [0.1, 0.4, 0.43, 0.45, 0.5, 0.7],\r\n",
    "        'subsample': [0.5, 0.75, 1.0],\r\n",
    "        'colsample_bytree': [0.1, 0.5, 0.7, 0.8, 0.9, 1.0],\r\n",
    "        'colsample_bylevel': [0.1, 0.5, 0.8, 1.0],\r\n",
    "        'colsample_bynode': [0.1, 0.3, 0.5, 0.75, 0.9, 1.0],\r\n",
    "        'min_child_weight': [0.1, 0.5, 1.0, 3.0, 5.0, 7.0, 10.0, 15, 20],\r\n",
    "        'gamma': [0, 0.1, 0.15, 0.25, 0.5, 1],\r\n",
    "        'reg_lambda': [0.1, 0.5, 1.0, 5.0, ],\r\n",
    "        'reg_alpha': [0.1, 1.0, 5.0, 10.0],\r\n",
    "        'n_estimators': [100, 200, 300, 400, 500]}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "rs_clf = RandomizedSearchCV(XGBClassifier(), param_grid, verbose=2, scoring='f1_micro', cv=2, n_iter=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# rs_clf.fit(train_values, train_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# rs_clf.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "modelo = xgb.XGBClassifier(eta=0.4, n_estimators=400, subsample=1, \r\n",
    "                        reg_lambda=0.5, reg_alpha= 5, min_child_weight=3, max_depth=6, \r\n",
    "                        gamma=0.25, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "modelo.fit(train_values, train_labels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[15:57:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=0.5, colsample_bytree=1, eta=0.4, gamma=0.25,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.400000006, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=400, n_jobs=16, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=5,\n",
       "              reg_lambda=0.5, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "predictions = modelo.predict(validation_values) \r\n",
    "predictionsProba = modelo.predict_proba(validation_values) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "f1_score(validation_labels, predictions, average='micro')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7467507799279358"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una parte del proceso de desarrollo, no es exaustivo ni completo, solo algunas cosas que fueron siendo anotadas aca\r\n",
    "\r\n",
    "- 0.7237961481345045 normal features + geolevel 1 y 2 categorizado\r\n",
    "- 0.7129174485132443 selected features del anterior\r\n",
    "- 0.7275720354104551 new best, normal features. solo get dummys\r\n",
    "- 0.6717856032785753 normal features, selected. muy pocas categorias\r\n",
    "- 0.7248322147651006 normal features + geoLevel1 categorizado.\r\n",
    "- 0.6693450907709487 normal features + geolevel1 categorizado selected (solo 24 features)\r\n",
    "- 0.7398513436249285 score con todos los datos, con normal features + geolevel1 categorizado\r\n",
    "- Score real con lo de arriba: 0.7249\r\n",
    "- cambiar a dart no trae mejoras\r\n",
    "- cambiar a gblinear empeora\r\n",
    "\r\n",
    "- 0.7262596843450332 baseline \r\n",
    "- 0.738093867636732 eta 0.7\r\n",
    "- 0.7415474230720526 eta 0.7, n_estimators = 200\r\n",
    "- 0.7438152578079132 eta=0.43, n_estimators=300\r\n",
    "\r\n",
    "- usar randomized search y probar otros algoritmos\r\n",
    "- 0.744264220014505 params3\r\n",
    "- 0.7459334384749099 con 50 geolevels\r\n",
    "- 0.7453002866451012 ultima entrega con mean encoding "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "submission_format = pd.read_csv('submission_format.csv', index_col='building_id')\r\n",
    "my_submission = pd.DataFrame(data=predictions,\r\n",
    "                             columns=submission_format.columns,\r\n",
    "                             index=submission_format.index)\r\n",
    "my_submission.to_csv('submissionXgBoost_meanEncoding_geo2.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "df_prediction_xgBoost = pd.DataFrame(predictionsProba, columns = ['xgb1', 'xgb2', 'xgb3'])\r\n",
    "df_prediction_xgBoost.to_csv('xgBoostPredictionProbaTrain.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "88ce181b8cd0eb812200b9b411a136cd9c7c972aa91497a27a228178326dd49d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}